ğŸ® AI-Powered Gesture-Based Subway Surfer Controller for Hand-Disabled Players
ğŸ“˜ Full Project Description
The AI-Powered Gesture-Based Subway Surfer Controller is an innovative assistive technology solution designed to empower individuals with hand disabilities to enjoy computer games such as Subway Surfer without the need for traditional input devices like a keyboard or mouse.

This project leverages Computer Vision and Artificial Intelligence to convert head movements into game control commands using only a webcam. Built using Python, the system integrates OpenCV for video capture and image processing, MediaPipe for real-time face and head landmark detection, and PyAutoGUI for simulating keypress events.

The primary goal of this project is to make fast-paced games more accessible, inclusive, and fun for players with physical limitations by providing a hands-free gaming experience.

ğŸ” How It Works
Camera Input: The webcam continuously captures the user's video stream.

Head Detection: MediaPipe detects facial landmarks and identifies the position of the user's head in real-time.

Screen Zone Division: The live camera frame is virtually split into four zones â€” up, down, left, and right.

Gesture Recognition: Based on the movement of the head into a specific zone:

Moving right â†’ triggers the Right Arrow key.

Moving left â†’ triggers the Left Arrow key.

Moving up â†’ triggers the Up Arrow key (for jumping).

Moving down â†’ triggers the Down Arrow key (for sliding).

Simulated Keypress: PyAutoGUI sends the corresponding keyboard command to control the character in the game.

âœ¨ Key Features
ğŸ¥ Real-Time Head Tracking
Uses MediaPipe's efficient face mesh detection to track head position with high accuracy.

ğŸ§  Gesture-Based Interaction
Converts intuitive head movements into actual in-game actions.

âŒ¨ï¸ Automatic Keyboard Simulation
Seamlessly triggers keyboard arrow keys using PyAutoGUI based on user gestures.

âš¡ Lightweight & Efficient
Designed to run smoothly on standard laptops and PCs with minimal setup.

â™¿ Accessibility-Focused
Aimed at gamers who cannot use their hands or traditional input devices.

ğŸš€ No External Hardware Required
Only needs a built-in or external webcamâ€”no fancy sensors or controllers.

ğŸ“¦ Technologies & Libraries Used
Python 3.x â€“ Core programming language for logic and integration.

OpenCV (cv2) â€“ For video capture, image manipulation, and frame processing.

MediaPipe â€“ Googleâ€™s ML pipeline for detecting face mesh and tracking head position.

PyAutoGUI â€“ For simulating keypress events based on gesture recognition.

ğŸ”§ Potential Use Cases & Future Enhancements
ğŸ® Extending to other games that use arrow key inputs.

ğŸ“± Porting to mobile platforms for broader accessibility.

ğŸ¤– Adding voice control or additional gestures for more game actions.

ğŸ§  Enhancing gesture accuracy using machine learning models for better user experience.

Let me know if youâ€™d like a polished version of this for a project report, portfolio website, or academic submission â€” I can format it accordingly!
