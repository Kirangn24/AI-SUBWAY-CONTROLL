ğŸ® AI-Powered Gesture-Based Subway Surfer Controller
A hands-free, AI-driven game controller that empowers hand-disabled gamers to play Subway Surfer using only head movements via webcam.

ğŸ“˜ Project Overview
This project transforms head gestures into game controls using computer vision and AI, allowing individuals with hand disabilities to enjoy games without a keyboard or mouse.

Built with Python, OpenCV, MediaPipe, and PyAutoGUI, the system detects head movement in real-time and simulates keyboard inputs for intuitive, hands-free gameplay.

ğŸ” How It Works
ğŸ¥ Webcam Input â€“ Captures real-time video of the player's face.

ğŸ§  Head Tracking â€“ MediaPipe detects facial landmarks to identify head orientation.

ğŸ•¹ï¸ Gesture Zones â€“ The screen is split into four directional zones.

âŒ¨ï¸ Simulated Controls â€“ PyAutoGUI triggers arrow keys based on detected gestures:

Left head tilt â†’ Left Arrow

Right head tilt â†’ Right Arrow

Head up â†’ Up Arrow (Jump)

Head down â†’ Down Arrow (Slide)

âœ¨ Features
Real-time, accurate head movement tracking

Fully hands-free control system

Easy setupâ€”just Python + Webcam

No external sensors or devices required

Accessibility-first design for inclusive gaming

ğŸ“¦ Tech Stack
Python 3.x

OpenCV

MediaPipe

PyAutoGUI

ğŸš€ Future Enhancements
Expand to more games and controls

Add gesture calibration UI

Integrate voice commands

Port to mobile or browser environments
